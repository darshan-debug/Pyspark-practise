{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02edc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import date,datetime\n",
    "spark=SparkSession.builder.appName(\"pyspark practise-1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfdd7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------------+----------+-------------------+\n",
      "| id|   name|salary|self_employed|start_date|         last_login|\n",
      "+---+-------+------+-------------+----------+-------------------+\n",
      "|  1|  alice|100.78|         true|2025-05-12|2025-05-12 14:30:00|\n",
      "|  2|    bob|200.89|        false|2025-01-01|2025-01-01 14:30:00|\n",
      "|  3|charlie| 30.87|         true|2026-01-01|2025-01-01 14:30:00|\n",
      "+---+-------+------+-------------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- self_employed: boolean (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- last_login: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='alice', salary=100.77999877929688, self_employed=True, start_date=datetime.date(2025, 5, 12), last_login=datetime.datetime(2025, 5, 12, 14, 30)),\n",
       " Row(id=2, name='bob', salary=200.88999938964844, self_employed=False, start_date=datetime.date(2025, 1, 1), last_login=datetime.datetime(2025, 1, 1, 14, 30))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(1,'alice',100.78,True,date(2025,5,12),datetime(2025,5,12,14,30,0)),(2,'bob',200.89,False,date(2025,1,1),datetime(2025,1,1,14,30,0)), (3,'charlie',30.870,True,date(2026,1,1),datetime(2025,1,1,14,30,0))]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema=\"id int,name string,salary float,self_employed boolean,start_date date,last_login timestamp\")\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3240707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, event_date=datetime.date(2025, 1, 1), count=2),\n",
       " Row(id=1, event_date=datetime.date(2025, 1, 2), count=3),\n",
       " Row(id=2, event_date=datetime.date(2025, 1, 2), count=5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file without header with schema\n",
    "\n",
    "df2=spark.read.format('csv').option('header','false').schema('id int,event_date date,count int').load('./csvFiles/sample1.csv')\n",
    "\n",
    "df2.printSchema()\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8681d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- roll_no: integer (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(roll_no=1, admission_date=datetime.date(2025, 1, 1), marks=2),\n",
       " Row(roll_no=1, admission_date=datetime.date(2025, 1, 2), marks=3),\n",
       " Row(roll_no=2, admission_date=datetime.date(2025, 1, 2), marks=5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file with header with schema\n",
    "df2=spark.read.format('csv').option('header','true').option('inferSchema','true').load('./csvFiles/sample2.csv')\n",
    "df2.printSchema()\n",
    "df2.head(3)\n",
    "# df2=spark.read.format('csv').option('header','true').option('inferSchema','true').option('delimiter',',').load('sample2.csv')\n",
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='dk', state='active')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading JSON file with/without schema\n",
    "#df=spark.read.format('json').option('inferSchema','true').option('multiline','true').load('./csvFiles/sample1.json')\n",
    "df=spark.read.format('json').schema('name string,state string').option('multiline','true').load('./csvFiles/sample1.json')\n",
    "#schema('name string,state string')\n",
    "df.printSchema()\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e7d5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe df1:\n",
      "Selecting specific columns from df1:\n",
      "+--------+--------+--------+---------+\n",
      "|max_col1|min_col2|avg_col3|sum(col1)|\n",
      "+--------+--------+--------+---------+\n",
      "|       7|       2|     6.0|       12|\n",
      "+--------+--------+--------+---------+\n",
      "\n",
      "+-----------------------+--------------------------+\n",
      "|least(col1, col2, col3)|greatest(col1, col2, col3)|\n",
      "+-----------------------+--------------------------+\n",
      "|                      1|                         3|\n",
      "|                      4|                         6|\n",
      "|                      7|                         9|\n",
      "+-----------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting columns from dataframe\n",
    "df1=spark.createDataFrame([[1,2,3],[4,5,6],[7,8,9]],schema=\"col1 int,col2 int,col3 int\")\n",
    "print(\"Dataframe df1:\")\n",
    "#df1.show()\n",
    "print(\"Selecting specific columns from df1:\")\n",
    "#df1.select('col1','col3').show()\n",
    "from pyspark.sql.functions import col\n",
    "# Using col function to select columns\n",
    "#df1.select(col('col1').alias('col1_alias'),col('col2').alias('col2_alias')).show()\n",
    "\n",
    "#simple aggregation functions\n",
    "from pyspark.sql.functions import max,min,avg,sum,least,greatest\n",
    "df1.select(max(col('col1')).alias('max_col1'),min(col('col2')).alias('min_col2'),avg(col('col3')).alias('avg_col3'),sum('col1')).show()\n",
    "\n",
    "df1.select(least('col1','col2','col3'),greatest('col1','col2','col3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2163488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "|    name|maths|science|english|remarks|total_marks|        percentage|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "|student1|   45|     33|     24|   good|        102| 81.33333333333333|\n",
      "|student2|    5|      3|     20|    bad|         28|18.666666666666668|\n",
      "|student3|   50|     50|     50|   good|        150|             100.0|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "|    name|maths|science|english|remarks|total_marks|        percentage|highest_marks|lowest_marks|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "|student1|   45|     33|     24|   good|        102| 81.33333333333333|           45|          24|\n",
      "|student2|    5|      3|     20|    bad|         28|18.666666666666668|           20|           3|\n",
      "|student3|   50|     50|     50|   good|        150|             100.0|           50|          50|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to existing dataframe\n",
    "df1=spark.createDataFrame([['student1',45,33,24,'good'],['student2',5,3,20,'bad'],['student3',50,50,50,'good']],schema='name string,maths int,science int,english int,remarks string')\n",
    "df1.show()\n",
    "# Adding a new column with total marks \n",
    "# df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english'))\n",
    "# df2.show()\n",
    "# Adding a new column with total marks and percentage\n",
    "# df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english')).withColumn('percentage',(col('total_marks')*100)/150)\n",
    "# df2.show()\n",
    "\n",
    "#1) using withColumn to add a new column\n",
    "\n",
    "# Adding a new column with total marks and percentage calculated in a complex way\n",
    "from pyspark.sql.functions import when,least,lit\n",
    "df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english')).withColumn('percentage',\n",
    "                                                                                        when(col('remarks')=='good',(least(col('total_marks')+20,lit(150))*100)/150)\n",
    "                                                                                        .when(col('remarks')=='bad',(col('total_marks')*100)/150)\n",
    "                                                                                        .otherwise(0)\n",
    "                                                                                        )\n",
    "df2.show()\n",
    "#note: least function returns the least value among the columns passed to it.\n",
    "#note: lit function is used to create a column with a literal/constant value.\n",
    "#note: when-otherwise in pyspark is similar to if-else condition in python.\n",
    "\n",
    "#1) using withColumns to add a new column\n",
    "\n",
    "from pyspark.sql.functions import greatest,least\n",
    "# Adding a new column with highest and lowest marks among the subjects\n",
    "df3=df2.withColumns({\n",
    "    'highest_marks':greatest(col('maths'),col('science'),col('english')),\n",
    "    'lowest_marks':least(col('maths'),col('science'),col('english'))\n",
    "})\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834b6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   1|\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping a columns from dataframe\n",
    "from pyspark.sql.functions import col\n",
    "df1=spark.createDataFrame([[1,2,3],[4,5,6]],schema=\"col1 int,col2 int,col3 int\")\n",
    "df1.drop(col('col2'),col('col3')).show()  # Dropping a column from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BASIC FILTERING\n",
    "df1=spark.createDataFrame([['student1',45,33,24,'good'],['student2',5,3,20,'bad'],['student3',50,50,50,'good']],schema='name string,maths int,science int,english int,remarks string')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "df1.filter(col('remarks')=='good').show()\n",
    "## Multiple conditions require parentheses around each condition\n",
    "df1.filter((col('remarks')=='good') & (col('science')>45)).show()\n",
    "\n",
    "df1.filter((col('remarks')=='bad') | (col('english')<25)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "| R123|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "names starting with R\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|   200|\n",
      "| R123|   100|\n",
      "+-----+------+\n",
      "\n",
      "names having 5 letters\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "+-----+------+\n",
      "\n",
      "names having 5 letters-using regex\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "+-----+------+\n",
      "\n",
      "names having only small letters\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|sunny|    40|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "names starting with capital letter, and followed by 3 digits\n",
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|R123|   100|\n",
      "+----+------+\n",
      "\n",
      "names having exactly 4 small letters\n",
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|xxxx|   100|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STRING FILTERING\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',200],['sunny',40],['R123',100],['xxxx',100]],schema='name string,salary int')\n",
    "from pyspark.sql.functions import col\n",
    "df1.show()\n",
    "# like operator is similar to sql like operator\n",
    "print('names starting with R')\n",
    "df1.filter(col('name').like('R%')).show()  # names starting with R\n",
    "print('names having 5 letters')\n",
    "df1.filter(col('name').like('_____')).show()  # names having 5 letters\n",
    "# regex filtering\n",
    "print('names having 5 letters-using regex')\n",
    "df1.filter(col('name').rlike('^.{5}$')).show()  # names having 5 letters\n",
    "print('names having only small letters')\n",
    "df1.filter(col('name').rlike('^[a-z]+$')).show() #names having only small letters\n",
    "print('names starting with capital letter, and followed by 3 digits')\n",
    "df1.filter(col('name').rlike('^[A-Z][0-9]{3}$')).show() #names starting with capital letter, and followed by 3 digits\n",
    "print('names having exactly 4 small letters')\n",
    "df1.filter(col('name').rlike('^[a-z]{4}$')).show()\n",
    "\n",
    "# Explanation of regex patterns used above:\n",
    "# ^: Matches the beginning of the string.\n",
    "\n",
    "# $: Matches the end of the string.\n",
    "\n",
    "# .: Matches any single character.\n",
    "\n",
    "# *: Matches the preceding character zero or more times.\n",
    "\n",
    "# +: Matches the preceding character one or more times.\n",
    "\n",
    "# [ ]: Matches any one of the characters inside the brackets.\n",
    "\n",
    "# |: Acts as an OR condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "iltering rows where salary is null\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|  NULL|\n",
      "+-----+------+\n",
      "\n",
      "Filtering rows where salary is not null\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Dropping all rows with null values in any column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Dropping any rows with null values in name column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling all null values, for all columns with a specific value: 'N/A' for all columns  \n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling all null values, for all columns with a specific value: 0 for all columns  \n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|     0|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "modified df:\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "| NULL|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling null values in specific columns with specific values\n",
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|   Ravi|   100|\n",
      "|  Rahul|     0|\n",
      "|Unknown|    40|\n",
      "|     kk|   100|\n",
      "|   xxxx|   100|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dealing with null values\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',None],['sunny',40],['kk',100],['xxxx',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "print(\"iltering rows where salary is null\")\n",
    "df1.filter(col('salary').isNull()).show()  # Filtering rows where salary is null\n",
    "print(\"Filtering rows where salary is not null\")\n",
    "df1.filter(col('salary').isNotNull()).show()  # Filtering rows where salary is not null\n",
    "print(\"Dropping all rows with null values in any column\")\n",
    "df1.dropna().show()\n",
    "print(\"Dropping any rows with null values in name column\")\n",
    "df1.dropna(subset=['name']).show() #subset needs a list of strings, so don't use col() here\n",
    "\n",
    "print(\"Filling all null values, for all columns with a specific value: 'N/A' for all columns  \")\n",
    "df1.fillna('N/A').show() #this will NOT fill nulls in int column, since 'N/A' is string\n",
    "\n",
    "print(\"Filling all null values, for all columns with a specific value: 0 for all columns  \")\n",
    "df1.fillna(0).show() #this will fill nulls in int column, since 0 is int\n",
    "\n",
    "print('modified df:')\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',None],[None,40],['kk',100],['xxxx',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "print(\"Filling null values in specific columns with specific values\")\n",
    "df1.fillna({\n",
    "    'name':'Unknown',\n",
    "    'salary':0\n",
    "}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84300254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "Removing duplicate rows from dataframe\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "Removing duplicate rows based on specific column(s), ie name column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([['rr',100],['rr',90],['sunny',90],['kk',100],['kk',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Removing duplicate rows from dataframe\")\n",
    "df1.distinct().show()\n",
    "\n",
    "print(\"Removing duplicate rows based on specific column(s), ie name column\")\n",
    "df1.dropDuplicates(subset=['name']).show() # gives more control, you can pass a list of columns based on which duplicates should be removed.\n",
    "#if subset is not passed, it behaves like distinct()\n",
    "\n",
    "df1.filter(col('name').isin(['rr','sunny'])).show()  # Filtering rows where name is in the given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d208b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
