{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02edc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import date,datetime\n",
    "spark=SparkSession.builder.appName(\"pyspark practise-1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdd7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------------+----------+-------------------+\n",
      "| id|   name|salary|self_employed|start_date|         last_login|\n",
      "+---+-------+------+-------------+----------+-------------------+\n",
      "|  1|  alice|100.78|         true|2025-05-12|2025-05-12 14:30:00|\n",
      "|  2|    bob|200.89|        false|2025-01-01|2025-01-01 14:30:00|\n",
      "|  3|charlie| 30.87|         true|2026-01-01|2025-01-01 14:30:00|\n",
      "+---+-------+------+-------------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- self_employed: boolean (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- last_login: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='alice', salary=100.77999877929688, self_employed=True, start_date=datetime.date(2025, 5, 12), last_login=datetime.datetime(2025, 5, 12, 14, 30)),\n",
       " Row(id=2, name='bob', salary=200.88999938964844, self_employed=False, start_date=datetime.date(2025, 1, 1), last_login=datetime.datetime(2025, 1, 1, 14, 30))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(1,'alice',100.78,True,date(2025,5,12),datetime(2025,5,12,14,30,0)),(2,'bob',200.89,False,date(2025,1,1),datetime(2025,1,1,14,30,0)), (3,'charlie',30.870,True,date(2026,1,1),datetime(2025,1,1,14,30,0))]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema=\"id int,name string,salary float,self_employed boolean,start_date date,last_login timestamp\")\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3240707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, event_date=datetime.date(2025, 1, 1), count=2),\n",
       " Row(id=1, event_date=datetime.date(2025, 1, 2), count=3),\n",
       " Row(id=2, event_date=datetime.date(2025, 1, 2), count=5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file without header with schema\n",
    "\n",
    "df2=spark.read.format('csv').option('header','false').schema('id int,event_date date,count int').load('./csvFiles/sample1.csv')\n",
    "\n",
    "df2.printSchema()\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8681d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- roll_no: integer (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(roll_no=1, admission_date=datetime.date(2025, 1, 1), marks=2),\n",
       " Row(roll_no=1, admission_date=datetime.date(2025, 1, 2), marks=3),\n",
       " Row(roll_no=2, admission_date=datetime.date(2025, 1, 2), marks=5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file with header with schema\n",
    "df2=spark.read.format('csv').option('header','true').option('inferSchema','true').load('./csvFiles/sample2.csv')\n",
    "df2.printSchema()\n",
    "df2.head(3)\n",
    "# df2=spark.read.format('csv').option('header','true').option('inferSchema','true').option('delimiter',',').load('sample2.csv')\n",
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='dk', state='active')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading JSON file with/without schema\n",
    "#df=spark.read.format('json').option('inferSchema','true').option('multiline','true').load('./csvFiles/sample1.json')\n",
    "df=spark.read.format('json').schema('name string,state string').option('multiline','true').load('./csvFiles/sample1.json')\n",
    "#schema('name string,state string')\n",
    "df.printSchema()\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7d5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe df1:\n",
      "Selecting specific columns from df1:\n",
      "+--------+--------+--------+---------+--------------------+\n",
      "|max_col1|min_col2|avg_col3|sum(col1)|count(DISTINCT col1)|\n",
      "+--------+--------+--------+---------+--------------------+\n",
      "|       7|       2|     6.0|       12|                   3|\n",
      "+--------+--------+--------+---------+--------------------+\n",
      "\n",
      "+-----------------------+--------------------------+\n",
      "|least(col1, col2, col3)|greatest(col1, col2, col3)|\n",
      "+-----------------------+--------------------------+\n",
      "|                      1|                         3|\n",
      "|                      4|                         6|\n",
      "|                      7|                         9|\n",
      "+-----------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting columns from dataframe\n",
    "df1=spark.createDataFrame([[1,2,3],[4,5,6],[7,8,9]],schema=\"col1 int,col2 int,col3 int\")\n",
    "print(\"Dataframe df1:\")\n",
    "#df1.show()\n",
    "print(\"Selecting specific columns from df1:\")\n",
    "#df1.select('col1','col3').show()\n",
    "from pyspark.sql.functions import col\n",
    "# Using col function to select columns\n",
    "#df1.select(col('col1').alias('col1_alias'),col('col2').alias('col2_alias')).show()\n",
    "\n",
    "#simple aggregation functions\n",
    "from pyspark.sql.functions import max,min,avg,sum,least,greatest,count_distinct\n",
    "df1.select(max(col('col1')).alias('max_col1'),min(col('col2')).alias('min_col2'),avg(col('col3')).alias('avg_col3'),sum('col1'),count_distinct('col1')).show()\n",
    "\n",
    "df1.select(least('col1','col2','col3'),greatest('col1','col2','col3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2163488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "|    name|maths|science|english|remarks|total_marks|        percentage|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "|student1|   45|     33|     24|   good|        102| 81.33333333333333|\n",
      "|student2|    5|      3|     20|    bad|         28|18.666666666666668|\n",
      "|student3|   50|     50|     50|   good|        150|             100.0|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "|    name|maths|science|english|remarks|total_marks|        percentage|highest_marks|lowest_marks|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "|student1|   45|     33|     24|   good|        102| 81.33333333333333|           45|          24|\n",
      "|student2|    5|      3|     20|    bad|         28|18.666666666666668|           20|           3|\n",
      "|student3|   50|     50|     50|   good|        150|             100.0|           50|          50|\n",
      "+--------+-----+-------+-------+-------+-----------+------------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to existing dataframe\n",
    "df1=spark.createDataFrame([['student1',45,33,24,'good'],['student2',5,3,20,'bad'],['student3',50,50,50,'good']],schema='name string,maths int,science int,english int,remarks string')\n",
    "df1.show()\n",
    "# Adding a new column with total marks \n",
    "# df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english'))\n",
    "# df2.show()\n",
    "# Adding a new column with total marks and percentage\n",
    "# df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english')).withColumn('percentage',(col('total_marks')*100)/150)\n",
    "# df2.show()\n",
    "\n",
    "#1) using withColumn to add a new column\n",
    "\n",
    "# Adding a new column with total marks and percentage calculated in a complex way\n",
    "from pyspark.sql.functions import when,least,lit\n",
    "df2=df1.withColumn('total_marks',col('maths')+col('science')+col('english')).withColumn('percentage',\n",
    "                                                                                        when(col('remarks')=='good',(least(col('total_marks')+20,lit(150))*100)/150)\n",
    "                                                                                        .when(col('remarks')=='bad',(col('total_marks')*100)/150)\n",
    "                                                                                        .otherwise(0)\n",
    "                                                                                        )\n",
    "df2.show()\n",
    "#note: least function returns the least value among the columns passed to it.\n",
    "#note: lit function is used to create a column with a literal/constant value.\n",
    "#note: when-otherwise in pyspark is similar to if-else condition in python.\n",
    "\n",
    "#1) using withColumns to add a new column\n",
    "\n",
    "from pyspark.sql.functions import greatest,least\n",
    "# Adding a new column with highest and lowest marks among the subjects\n",
    "df3=df2.withColumns({\n",
    "    'highest_marks':greatest(col('maths'),col('science'),col('english')),\n",
    "    'lowest_marks':least(col('maths'),col('science'),col('english'))\n",
    "})\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834b6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   1|\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping a columns from dataframe\n",
    "from pyspark.sql.functions import col\n",
    "df1=spark.createDataFrame([[1,2,3],[4,5,6]],schema=\"col1 int,col2 int,col3 int\")\n",
    "df1.drop(col('col2'),col('col3')).show()  # Dropping a column from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student3|   50|     50|     50|   good|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n",
      "+--------+-----+-------+-------+-------+\n",
      "|    name|maths|science|english|remarks|\n",
      "+--------+-----+-------+-------+-------+\n",
      "|student1|   45|     33|     24|   good|\n",
      "|student2|    5|      3|     20|    bad|\n",
      "+--------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BASIC FILTERING\n",
    "df1=spark.createDataFrame([['student1',45,33,24,'good'],['student2',5,3,20,'bad'],['student3',50,50,50,'good']],schema='name string,maths int,science int,english int,remarks string')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "df1.filter(col('remarks')=='good').show()\n",
    "## Multiple conditions require parentheses around each condition\n",
    "df1.filter((col('remarks')=='good') & (col('science')>45)).show()\n",
    "\n",
    "df1.filter((col('remarks')=='bad') | (col('english')<25)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "| R123|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "names starting with R\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|   200|\n",
      "| R123|   100|\n",
      "+-----+------+\n",
      "\n",
      "names having 5 letters\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "+-----+------+\n",
      "\n",
      "names having 5 letters-using regex\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|   200|\n",
      "|sunny|    40|\n",
      "+-----+------+\n",
      "\n",
      "names having only small letters\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|sunny|    40|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "names starting with capital letter, and followed by 3 digits\n",
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|R123|   100|\n",
      "+----+------+\n",
      "\n",
      "names having exactly 4 small letters\n",
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|xxxx|   100|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STRING FILTERING\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',200],['sunny',40],['R123',100],['xxxx',100]],schema='name string,salary int')\n",
    "from pyspark.sql.functions import col\n",
    "df1.show()\n",
    "# like operator is similar to sql like operator\n",
    "print('names starting with R')\n",
    "df1.filter(col('name').like('R%')).show()  # names starting with R\n",
    "print('names having 5 letters')\n",
    "df1.filter(col('name').like('_____')).show()  # names having 5 letters\n",
    "# regex filtering\n",
    "print('names having 5 letters-using regex')\n",
    "df1.filter(col('name').rlike('^.{5}$')).show()  # names having 5 letters\n",
    "print('names having only small letters')\n",
    "df1.filter(col('name').rlike('^[a-z]+$')).show() #names having only small letters\n",
    "print('names starting with capital letter, and followed by 3 digits')\n",
    "df1.filter(col('name').rlike('^[A-Z][0-9]{3}$')).show() #names starting with capital letter, and followed by 3 digits\n",
    "print('names having exactly 4 small letters')\n",
    "df1.filter(col('name').rlike('^[a-z]{4}$')).show()\n",
    "\n",
    "# Explanation of regex patterns used above:\n",
    "# ^: Matches the beginning of the string.\n",
    "\n",
    "# $: Matches the end of the string.\n",
    "\n",
    "# .: Matches any single character.\n",
    "\n",
    "# *: Matches the preceding character zero or more times.\n",
    "\n",
    "# +: Matches the preceding character one or more times.\n",
    "\n",
    "# [ ]: Matches any one of the characters inside the brackets.\n",
    "\n",
    "# |: Acts as an OR condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "iltering rows where salary is null\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Rahul|  NULL|\n",
      "+-----+------+\n",
      "\n",
      "Filtering rows where salary is not null\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Dropping all rows with null values in any column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Dropping any rows with null values in name column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling all null values, for all columns with a specific value: 'N/A' for all columns  \n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling all null values, for all columns with a specific value: 0 for all columns  \n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|     0|\n",
      "|sunny|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "modified df:\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| Ravi|   100|\n",
      "|Rahul|  NULL|\n",
      "| NULL|    40|\n",
      "|   kk|   100|\n",
      "| xxxx|   100|\n",
      "+-----+------+\n",
      "\n",
      "Filling null values in specific columns with specific values\n",
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|   Ravi|   100|\n",
      "|  Rahul|     0|\n",
      "|Unknown|    40|\n",
      "|     kk|   100|\n",
      "|   xxxx|   100|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dealing with null values\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',None],['sunny',40],['kk',100],['xxxx',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "print(\"iltering rows where salary is null\")\n",
    "df1.filter(col('salary').isNull()).show()  # Filtering rows where salary is null\n",
    "print(\"Filtering rows where salary is not null\")\n",
    "df1.filter(col('salary').isNotNull()).show()  # Filtering rows where salary is not null\n",
    "print(\"Dropping all rows with null values in any column\")\n",
    "df1.dropna().show()\n",
    "print(\"Dropping any rows with null values in name column\")\n",
    "df1.dropna(subset=['name']).show() #subset needs a list of strings, so don't use col() here\n",
    "\n",
    "print(\"Filling all null values, for all columns with a specific value: 'N/A' for all columns  \")\n",
    "df1.fillna('N/A').show() #this will NOT fill nulls in int column, since 'N/A' is string\n",
    "\n",
    "print(\"Filling all null values, for all columns with a specific value: 0 for all columns  \")\n",
    "df1.fillna(0).show() #this will fill nulls in int column, since 0 is int\n",
    "\n",
    "print('modified df:')\n",
    "df1=spark.createDataFrame([['Ravi',100],['Rahul',None],[None,40],['kk',100],['xxxx',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "print(\"Filling null values in specific columns with specific values\")\n",
    "df1.fillna({\n",
    "    'name':'Unknown',\n",
    "    'salary':0\n",
    "}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84300254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "Removing duplicate rows from dataframe\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "Removing duplicate rows based on specific column(s), ie name column\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    90|\n",
      "|sunny|    90|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([['rr',100],['rr',90],['sunny',90],['kk',100],['kk',100]],schema='name string,salary int')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Removing duplicate rows from dataframe\")\n",
    "df1.distinct().show()\n",
    "\n",
    "print(\"Removing duplicate rows based on specific column(s), ie name column\")\n",
    "df1.dropDuplicates(subset=['name']).show() # gives more control, you can pass a list of columns based on which duplicates should be removed.\n",
    "#if subset is not passed, it behaves like distinct()\n",
    "\n",
    "df1.filter(col('name').isin(['rr','sunny'])).show()  # Filtering rows where name is in the given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d208b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|   rr|   100|\n",
      "|   rr|    50|\n",
      "|sunny|    90|\n",
      "|   kk|   100|\n",
      "|   kk|    10|\n",
      "+-----+------+\n",
      "\n",
      "+-----+-----------+\n",
      "| name|sum(salary)|\n",
      "+-----+-----------+\n",
      "|   rr|        150|\n",
      "|sunny|         90|\n",
      "|   kk|        110|\n",
      "+-----+-----------+\n",
      "\n",
      "+-----+-----------+------------+----------+----------+\n",
      "| name|salary_list|total_salary|max_salary|min_salary|\n",
      "+-----+-----------+------------+----------+----------+\n",
      "|   rr|  [100, 50]|         150|       100|        50|\n",
      "|sunny|       [90]|          90|        90|        90|\n",
      "|   kk|  [100, 10]|         110|       100|        10|\n",
      "+-----+-----------+------------+----------+----------+\n",
      "\n",
      "+----+------------+----------+----------+\n",
      "|name|total_salary|max_salary|min_salary|\n",
      "+----+------------+----------+----------+\n",
      "|  rr|         150|       100|        50|\n",
      "|  kk|         110|       100|        10|\n",
      "+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by\n",
    "df1=spark.createDataFrame([['rr',100],['rr',50],['sunny',90],['kk',100],['kk',10]],schema='name string,salary int')\n",
    "df1.show()\n",
    "from pyspark.sql.functions import col,sum,max,min,collect_list\n",
    "df1.groupby('name').sum('salary').show()  # Grouping by name and calculating sum of salary\n",
    "\n",
    "#VV IMP\n",
    "#using .agg() <-- this is more flexible and allows multiple aggregations, and also allows aliasing the aggregated columns\n",
    "\n",
    "df1.groupby(col('name')).agg(collect_list('salary').alias('salary_list'),sum('salary').alias('total_salary'),max('salary').alias('max_salary'),min('salary').alias('min_salary')).show()\n",
    "\n",
    "#find aggregated values , for name where total salary>=100\n",
    "\n",
    "df1.groupby(col('name')).agg(sum('salary').alias('total_salary'),max('salary').alias('max_salary'),min('salary').alias('min_salary')).filter(col('total_salary')>=100).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80454d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+-----+\n",
      "| id| name| id|subject|marks|\n",
      "+---+-----+---+-------+-----+\n",
      "|  1|alice|  1|  maths|   30|\n",
      "|  1|alice|  1|science|   20|\n",
      "|  2|  bob|  2|history|   50|\n",
      "+---+-----+---+-------+-----+\n",
      "\n",
      "+---+-------+----+-------+-----+\n",
      "| id|   name|  id|subject|marks|\n",
      "+---+-------+----+-------+-----+\n",
      "|  1|  alice|   1|science|   20|\n",
      "|  1|  alice|   1|  maths|   30|\n",
      "|  2|    bob|   2|history|   50|\n",
      "|  3|charlie|NULL|   NULL| NULL|\n",
      "+---+-------+----+-------+-----+\n",
      "\n",
      "+---+-------+---------+\n",
      "| id|   name|totalmark|\n",
      "+---+-------+---------+\n",
      "|  1|  alice|       50|\n",
      "|  2|    bob|       50|\n",
      "|  3|charlie|     NULL|\n",
      "+---+-------+---------+\n",
      "\n",
      "+---+-------+-----------+-------------+\n",
      "| id|   name|total_marks|     subjects|\n",
      "+---+-------+-----------+-------------+\n",
      "|  1|  alice|         50|maths,science|\n",
      "|  2|    bob|         50|      history|\n",
      "|  3|charlie|       NULL|         NULL|\n",
      "+---+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JOINS\n",
    "df1=spark.createDataFrame([[1,'alice'],[2,'bob'],[3,'charlie']],schema='id int,name string')\n",
    "df2=spark.createDataFrame([[1,'maths',30],[1,'science',20],[2,'history',50]],schema='id int,subject string,marks int')\n",
    "\n",
    "#inner join\n",
    "df1.join(df2,df1.id==df2.id,'inner').show()\n",
    "\n",
    "#left join\n",
    "df1.join(df2,df1.id==df2.id,'left').show()\n",
    "\n",
    "#its always better to add alias, before join, to avoid ambiguity\n",
    "from pyspark.sql.functions import sum\n",
    "df1=df1.alias('a')\n",
    "df2=df2.alias('b')\n",
    "df1.join(df2,df1.id==df2.id,'left').groupby('a.id','a.name').agg(sum('b.marks').alias('totalmark')).show()  # left anti join\n",
    "\n",
    "#trying temp view\n",
    "df1.createOrReplaceTempView('df1')\n",
    "df2.createOrReplaceTempView('df2')\n",
    "spark.sql(\"\"\"\n",
    "select a.id,a.name,sum(b.marks) as total_marks,string_agg(b.subject,',') within group(order by b.marks desc) as subjects\n",
    "from df1 a left join df2 b on a.id=b.id\n",
    "group by a.id,a.name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed09b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|sunny|    90|\n",
      "|   rr|    50|\n",
      "|   rr|   100|\n",
      "|   kk|    10|\n",
      "|   kk|   100|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame([['rr',100],['rr',50],['sunny',90],['kk',100],['kk',10]],schema='name string,salary int')\n",
    "from pyspark.sql.functions import asc,desc\n",
    "df1.orderBy(desc('name'),asc('salary')).show()  # Sorting by name ascending and salary ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e29f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+-------------------+\n",
      "|product_id|product_name|original_price|discount_percentage|\n",
      "+----------+------------+--------------+-------------------+\n",
      "|      P001|      Laptop|        1000.0|                 10|\n",
      "|      P002|       Phone|         800.0|                  5|\n",
      "|      P003|      Tablet|         600.0|                 15|\n",
      "|      P004|     Monitor|         300.0|                 20|\n",
      "|      P005|    Keyboard|         100.0|                 25|\n",
      "+----------+------------+--------------+-------------------+\n",
      "\n",
      "+----------+------------+-----------+\n",
      "|product_id|product_name|final_price|\n",
      "+----------+------------+-----------+\n",
      "|      P001|      Laptop|      900.0|\n",
      "|      P002|       Phone|      760.0|\n",
      "|      P003|      Tablet|      510.0|\n",
      "|      P004|     Monitor|      240.0|\n",
      "|      P005|    Keyboard|       75.0|\n",
      "+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.sparkplayground.com/pyspark-coding-interview-questions/discount-on-products\n",
    "from pyspark.sql.functions import col\n",
    "df=spark.read.format('csv').option('header','true').option('inferSchema','true').load('./csvFiles/productDiscount.csv')\n",
    "df.show()\n",
    "result_df=df.withColumns({\n",
    "    'final_price': col('original_price') * ( 1 - (col('discount_percentage')/100) )\n",
    "})\n",
    "\n",
    "result_df.select('product_id','product_name','final_price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460daa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|customer_id|order_id|            products|\n",
      "+-----------+--------+--------------------+\n",
      "|       C001|   O1001|[{Laptop, 1500}, ...|\n",
      "|       C002|   O1002|    [{Keyboard, 75}]|\n",
      "+-----------+--------+--------------------+\n",
      "\n",
      "+-----------+--------+------------+-------------+\n",
      "|customer_id|order_id|product_name|product_price|\n",
      "+-----------+--------+------------+-------------+\n",
      "|       C001|   O1001|      Laptop|         1500|\n",
      "|       C001|   O1001|       Mouse|           25|\n",
      "|       C002|   O1002|    Keyboard|           75|\n",
      "+-----------+--------+------------+-------------+\n",
      "\n",
      "+-----------+--------+------------+-------------+\n",
      "|customer_id|order_id|product_name|product_price|\n",
      "+-----------+--------+------------+-------------+\n",
      "|       C001|   O1001|      Laptop|         1500|\n",
      "|       C001|   O1001|       Mouse|           25|\n",
      "|       C002|   O1002|    Keyboard|           75|\n",
      "+-----------+--------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.sparkplayground.com/pyspark-coding-interview-questions/load-and-transform-data-json\n",
    "\n",
    "df=spark.read.format('json').option('multiline','true').load('./csvFiles/orders.json')\n",
    "df.show()\n",
    "df.createOrReplaceTempView('orders')\n",
    "#using inline\n",
    "spark.sql(\"\"\"\n",
    "select customer_id,order_id,prd.product_name,prd.product_price\n",
    "from orders\n",
    "lateral view outer inline(orders.products)  prd\n",
    "\"\"\").show()\n",
    "\n",
    "# using explode function\n",
    "spark.sql(\"\"\"\n",
    "select customer_id,order_id,prd.product_name,prd.product_price\n",
    "from orders\n",
    "lateral view outer explode(orders.products) as prd\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0336a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+-------------+\n",
      "|employee_id|employee_name|salary|department_id|\n",
      "+-----------+-------------+------+-------------+\n",
      "|          1|        Alice|  5000|            1|\n",
      "|          2|          Bob|  7000|            2|\n",
      "|          3|      Charlie|  4000|            1|\n",
      "|          4|        David|  6000|            2|\n",
      "|          5|          Eve|  8000|            3|\n",
      "|          6|          Kev|  9000|            3|\n",
      "|          7|          Mev| 10000|            3|\n",
      "|          8|          Mob| 12000|            2|\n",
      "+-----------+-------------+------+-------------+\n",
      "\n",
      "+-------------+---------------+\n",
      "|department_id|department_name|\n",
      "+-------------+---------------+\n",
      "|            1|             HR|\n",
      "|            2|    Engineering|\n",
      "|            3|        Finance|\n",
      "+-------------+---------------+\n",
      "\n",
      "+-------------+---------------+------+\n",
      "|employee_name|department_name|salary|\n",
      "+-------------+---------------+------+\n",
      "|        Alice|             HR|  5000|\n",
      "|          Mob|    Engineering| 12000|\n",
      "|          Mev|        Finance| 10000|\n",
      "+-------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.sparkplayground.com/pyspark-coding-interview-questions/employees-earning-more-than-average\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Employee DataFrame\n",
    "employee_data = [\n",
    "    (1, \"Alice\", 5000, 1),\n",
    "    (2, \"Bob\", 7000, 2),\n",
    "    (3, \"Charlie\", 4000, 1),\n",
    "    (4, \"David\", 6000, 2),\n",
    "    (5, \"Eve\", 8000, 3),\n",
    "    (6, \"Kev\", 9000, 3),\n",
    "    (7, \"Mev\", 10000, 3),\n",
    "    (8, \"Mob\", 12000, 2)\n",
    "]\n",
    "employee_columns = [\"employee_id\", \"employee_name\", \"salary\", \"department_id\"]\n",
    "emp_df = spark.createDataFrame(employee_data, employee_columns)\n",
    "\n",
    "# Department DataFrame\n",
    "department_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"Engineering\"),\n",
    "    (3, \"Finance\")\n",
    "]\n",
    "department_columns = [\"department_id\", \"department_name\"]\n",
    "dept_df = spark.createDataFrame(department_data, department_columns)\n",
    "\n",
    "# Display dataframes (optional)\n",
    "emp_df.show()\n",
    "dept_df.show()\n",
    "\n",
    "emp_df.createOrReplaceTempView('emp')\n",
    "dept_df.createOrReplaceTempView('dept')\n",
    "spark.sql(\"\"\"\n",
    "with cte1 as (\n",
    "select department_id,avg(salary) as avg_salary\n",
    "from emp\n",
    "group by department_id)\n",
    "select e.employee_name,d.department_name,e.salary\n",
    "from emp as e\n",
    "inner join dept as d on e.department_id=d.department_id\n",
    "inner join cte1 as c on e.department_id=c.department_id\n",
    "where e.salary>c.avg_salary\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+--------------------+\n",
      "|user_id|user_name|created_date|               email|\n",
      "+-------+---------+------------+--------------------+\n",
      "|      1|    Alice|  2023-05-10|   alice@example.com|\n",
      "|      1|    Alice|  2023-06-15|alice_new@example...|\n",
      "|      2|      Bob|  2023-07-01|     bob@example.com|\n",
      "|      3|  Charlie|  2023-05-20| charlie@example.com|\n",
      "|      3|  Charlie|  2023-06-25|charlie_updated@e...|\n",
      "|      4|    David|  2023-08-05|   david@example.com|\n",
      "+-------+---------+------------+--------------------+\n",
      "\n",
      "sorted by created_date desc:\n",
      "+-------+---------+------------+--------------------+\n",
      "|user_id|user_name|created_date|               email|\n",
      "+-------+---------+------------+--------------------+\n",
      "|      4|    David|  2023-08-05|   david@example.com|\n",
      "|      2|      Bob|  2023-07-01|     bob@example.com|\n",
      "|      3|  Charlie|  2023-06-25|charlie_updated@e...|\n",
      "|      1|    Alice|  2023-06-15|alice_new@example...|\n",
      "|      3|  Charlie|  2023-05-20| charlie@example.com|\n",
      "|      1|    Alice|  2023-05-10|   alice@example.com|\n",
      "+-------+---------+------------+--------------------+\n",
      "\n",
      "Removing duplicates based on user_id, keeping the most recent created_date entry:\n",
      "+-------+---------+------------+--------------------+\n",
      "|user_id|user_name|created_date|               email|\n",
      "+-------+---------+------------+--------------------+\n",
      "|      1|    Alice|  2023-06-15|alice_new@example...|\n",
      "|      2|      Bob|  2023-07-01|     bob@example.com|\n",
      "|      3|  Charlie|  2023-06-25|charlie_updated@e...|\n",
      "|      4|    David|  2023-08-05|   david@example.com|\n",
      "+-------+---------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.sparkplayground.com/pyspark-coding-interview-questions/remove-duplicates\n",
    "# Define schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import date\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"user_name\", StringType(), True),\n",
    "    StructField(\"created_date\", DateType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\", date(2023, 5, 10), \"alice@example.com\"),\n",
    "    (1, \"Alice\", date(2023, 6, 15), \"alice_new@example.com\"),\n",
    "    (2, \"Bob\", date(2023, 7, 1), \"bob@example.com\"),\n",
    "    (3, \"Charlie\", date(2023, 5, 20), \"charlie@example.com\"),\n",
    "    (3, \"Charlie\", date(2023, 6, 25), \"charlie_updated@example.com\"),\n",
    "    (4, \"David\", date(2023, 8, 5), \"david@example.com\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "user_df = spark.createDataFrame(data, schema)\n",
    "user_df.show()\n",
    "print(\"sorted by created_date desc:\")\n",
    "user_df.orderBy(col('created_date').desc()).show()\n",
    "print(\"Removing duplicates based on user_id, keeping the most recent created_date entry:\")\n",
    "user_df.orderBy(col('created_date').desc()).dropDuplicates(subset=['user_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d301c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------+\n",
      "|   category|sub_category|amount|\n",
      "+-----------+------------+------+\n",
      "|Electronics|      Laptop|  1000|\n",
      "|Electronics|      Laptop|  1200|\n",
      "|  Furniture|       Chair|   200|\n",
      "|  Furniture|       Chair|   150|\n",
      "|  Furniture|       Chair|   180|\n",
      "|  Furniture|       Chair|   200|\n",
      "|Electronics|      iPhone|   600|\n",
      "|Electronics|      iPhone|   400|\n",
      "+-----------+------------+------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|   category|sub_category|    amounts|\n",
      "+-----------+------------+-----------+\n",
      "|Electronics|      iPhone|    400,600|\n",
      "|  Furniture|       Chair|150,180,200|\n",
      "|Electronics|      Laptop|  1000,1200|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.sparkplayground.com/pyspark-coding-interview-questions/group-amounts\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sub_category\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Electronics\", \"Laptop\", 1000),\n",
    "    (\"Electronics\", \"Laptop\", 1200),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Furniture\", \"Chair\", 150),\n",
    "    (\"Furniture\", \"Chair\", 180),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Electronics\",\"iPhone\", 600),\n",
    "    (\"Electronics\",\"iPhone\", 400),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df=df.distinct() # remove duplicates first\n",
    "df.createOrReplaceTempView('products')\n",
    "spark.sql(\"\"\"\n",
    "select category,sub_category,string_agg(amount,',') within group (order by amount asc) as amounts\n",
    "from products\n",
    "group by category,sub_category\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7415d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+----------+\n",
      "| id|country|   state|amount|trans_date|\n",
      "+---+-------+--------+------+----------+\n",
      "|121|     US|approved|  1000|2018-12-18|\n",
      "|122|     US|declined|  2000|2018-12-19|\n",
      "|123|     US|approved|  2000|2019-01-01|\n",
      "|124|     DE|approved|  2000|2019-01-07|\n",
      "|125|     US|approved|   500|2018-12-20|\n",
      "|126|     US|declined|  1500|2019-01-05|\n",
      "|127|     DE|approved|  1800|2019-01-10|\n",
      "|128|     FR|declined|  1200|2019-02-15|\n",
      "|129|     FR|approved|  2500|2019-02-17|\n",
      "|130|     US|approved|  3000|2019-02-20|\n",
      "|131|     DE|declined|  2200|2019-03-05|\n",
      "|132|     FR|approved|  1000|2019-03-10|\n",
      "+---+-------+--------+------+----------+\n",
      "\n",
      "+-------+-------+-----------+--------------+------------------+\n",
      "|  month|country|trans_count|approved_count|trans_total_amount|\n",
      "+-------+-------+-----------+--------------+------------------+\n",
      "|2018-12|     US|          3|             2|              3500|\n",
      "|2019-01|     DE|          2|             2|              3800|\n",
      "|2019-01|     US|          2|             1|              3500|\n",
      "|2019-02|     FR|          2|             1|              3700|\n",
      "|2019-02|     US|          1|             1|              3000|\n",
      "|2019-03|     DE|          1|             0|              2200|\n",
      "|2019-03|     FR|          1|             1|              1000|\n",
      "+-------+-------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.sparkplayground.com/pyspark-coding-interview-questions/monthly-transaction-summary\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"amount\", IntegerType(), False),\n",
    "    StructField(\"trans_date\", StringType(), False),  # Initially as String\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (121, \"US\", \"approved\", 1000, \"2018-12-18\"),\n",
    "    (122, \"US\", \"declined\", 2000, \"2018-12-19\"),\n",
    "    (123, \"US\", \"approved\", 2000, \"2019-01-01\"),\n",
    "    (124, \"DE\", \"approved\", 2000, \"2019-01-07\"),\n",
    "    (125, \"US\", \"approved\", 500, \"2018-12-20\"),\n",
    "    (126, \"US\", \"declined\", 1500, \"2019-01-05\"),\n",
    "    (127, \"DE\", \"approved\", 1800, \"2019-01-10\"),\n",
    "    (128, \"FR\", \"declined\", 1200, \"2019-02-15\"),\n",
    "    (129, \"FR\", \"approved\", 2500, \"2019-02-17\"),\n",
    "    (130, \"US\", \"approved\", 3000, \"2019-02-20\"),\n",
    "    (131, \"DE\", \"declined\", 2200, \"2019-03-05\"),\n",
    "    (132, \"FR\", \"approved\", 1000, \"2019-03-10\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert trans_date column to DateType\n",
    "df = df.withColumn(\"trans_date\", to_date(col(\"trans_date\"), \"yyyy-MM-dd\"))\n",
    "# iff is not recognised in the version of spark ,I am using, so using case-when instead\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "df.createOrReplaceTempView('transactions')\n",
    "spark.sql(\"\"\"\n",
    "select date_format(trans_date,'yyyy-MM') as month,country,count(*) as trans_count,sum(\n",
    "case \n",
    "when state='approved' then 1\n",
    "else 0\n",
    "end) as approved_count,sum(amount) as trans_total_amount\n",
    "from transactions\n",
    "group by date_format(trans_date,'yyyy-MM'),country\n",
    "order by month,country\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a7aa83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|    player| runs|50s/100s|\n",
      "+----------+-----+--------+\n",
      "|Sachin-IND|18694|   93/49|\n",
      "| Ricky-AUS|11274|   66/31|\n",
      "|   Lara-WI|10222|   45/21|\n",
      "| Rahul-IND|10355|   95/11|\n",
      "|Jhonnty-SA| 7051|    43/5|\n",
      "|Hayden-AUS| 8722|   67/19|\n",
      "+----------+-----+--------+\n",
      "\n",
      "+---+-----------+\n",
      "|SRT|    country|\n",
      "+---+-----------+\n",
      "|IND|      India|\n",
      "|AUS|  Australia|\n",
      "| WI| WestIndies|\n",
      "| SA|SouthAfrica|\n",
      "+---+-----------+\n",
      "\n",
      "+----------+-------+-----+---+\n",
      "|playername|country| runs|sum|\n",
      "+----------+-------+-----+---+\n",
      "|    Sachin|    IND|18694|142|\n",
      "|     Ricky|    AUS|11274| 97|\n",
      "|     Rahul|    IND|10355|106|\n",
      "+----------+-------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.sparkplayground.com/pyspark-coding-interview-questions/player-statistics\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import split, col, expr\n",
    "\n",
    "# Create players_df\n",
    "players_data = [\n",
    "    (\"Sachin-IND\", 18694, \"93/49\"),\n",
    "    (\"Ricky-AUS\", 11274, \"66/31\"),\n",
    "    (\"Lara-WI\", 10222, \"45/21\"),\n",
    "    (\"Rahul-IND\", 10355, \"95/11\"),\n",
    "    (\"Jhonnty-SA\", 7051, \"43/5\"),\n",
    "    (\"Hayden-AUS\", 8722, \"67/19\")\n",
    "]\n",
    "\n",
    "players_df = spark.createDataFrame(players_data, [\"player\", \"runs\", \"50s/100s\"])\n",
    "\n",
    "# Create countries_df\n",
    "countries_data = [\n",
    "    (\"IND\", \"India\"),\n",
    "    (\"AUS\", \"Australia\"),\n",
    "    (\"WI\", \"WestIndies\"),\n",
    "    (\"SA\", \"SouthAfrica\")\n",
    "]\n",
    "\n",
    "countries_df = spark.createDataFrame(countries_data, [\"SRT\", \"country\"])\n",
    "\n",
    "#Your solution starts here\n",
    "players_df.show()\n",
    "countries_df.show()\n",
    "players_df.createOrReplaceTempView('players')\n",
    "countries_df.createOrReplaceTempView('countries')\n",
    "spark.sql(\"\"\"\n",
    "with cte1 as (\n",
    "select substring(player,1,instr(player,'-')-1) as playername,substring(player,instr(player,'-')+1,len(player))  as country,runs,cast(substring(`50s/100s`,1,instr(`50s/100s`,'/')-1) as int)+cast(substring(`50s/100s`,instr(`50s/100s`,'/')+1,len(50s/100s)) as int) as sum\n",
    "from players\n",
    ")\n",
    "select *\n",
    "from cte1\n",
    "where sum>95\n",
    "order by runs desc\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72985c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+-----------+\n",
      "|store_id|product_id| sale_date|quantity_sold|total_sales|\n",
      "+--------+----------+----------+-------------+-----------+\n",
      "|       1|       101|2025-05-10|            2|       25.0|\n",
      "|       1|       102|2025-05-10|            1|       15.0|\n",
      "|       1|       103|2025-05-11|            3|       30.0|\n",
      "|       2|       101|2025-05-10|            2|       40.0|\n",
      "+--------+----------+----------+-------------+-----------+\n",
      "\n",
      "+--------+----------+-----------------+\n",
      "|store_id| sale_date|daily_total_sales|\n",
      "+--------+----------+-----------------+\n",
      "|       1|2025-05-10|             40.0|\n",
      "|       1|2025-05-11|             30.0|\n",
      "|       2|2025-05-10|             40.0|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.sparkplayground.com/pyspark-coding-interview-questions/daily-total-sales\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, 101, \"2025-05-10\", 2, 25.00),\n",
    "    (1, 102, \"2025-05-10\", 1, 15.00),\n",
    "    (1, 103, \"2025-05-11\", 3, 30.00),\n",
    "    (2, 101, \"2025-05-10\", 2, 40.00)\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"sale_date\", \"quantity_sold\", \"total_sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.createOrReplaceTempView('sales')\n",
    "spark.sql(\"\"\"\n",
    "select store_id,sale_date,sum(total_sales) as  daily_total_sales\n",
    "from sales\n",
    "group by store_id,sale_date\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8740762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+-----------+\n",
      "|store_id|product_id| sale_date|quantity_sold|total_sales|\n",
      "+--------+----------+----------+-------------+-----------+\n",
      "|       1|       101|2025-05-10|            2|       25.0|\n",
      "|       2|       101|2025-05-10|            1|       15.0|\n",
      "|       1|       102|2025-05-10|            5|       50.0|\n",
      "|       3|       103|2025-05-10|            3|       30.0|\n",
      "|       2|       104|2025-05-10|            4|       45.0|\n",
      "|       1|       105|2025-05-10|            2|       60.0|\n",
      "|       1|       105|2025-05-10|            1|       15.0|\n",
      "|       1|       106|2025-05-10|            2|       10.0|\n",
      "|       1|       201|2025-05-11|            1|       20.0|\n",
      "|       2|       201|2025-05-11|            2|       40.0|\n",
      "|       2|       202|2025-05-11|            2|       40.0|\n",
      "|       3|       203|2025-05-11|            3|       35.0|\n",
      "|       1|       204|2025-05-11|            1|       25.0|\n",
      "|       2|       205|2025-05-11|            2|       30.0|\n",
      "|       1|       206|2025-05-11|            4|       50.0|\n",
      "+--------+----------+----------+-------------+-----------+\n",
      "\n",
      "+----------+----------+-----------+\n",
      "|product_id| sale_date|total_sales|\n",
      "+----------+----------+-----------+\n",
      "|       105|2025-05-10|       75.0|\n",
      "|       201|2025-05-11|       60.0|\n",
      "|       102|2025-05-10|       50.0|\n",
      "|       206|2025-05-11|       50.0|\n",
      "|       104|2025-05-10|       45.0|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.sparkplayground.com/pyspark-coding-interview-questions/top-products\n",
    "\n",
    "data = [\n",
    "    # 2025-05-10\n",
    "    (1, 101, \"2025-05-10\", 2, 25.00),\n",
    "    (2, 101, \"2025-05-10\", 1, 15.00),\n",
    "    (1, 102, \"2025-05-10\", 5, 50.00),\n",
    "    (3, 103, \"2025-05-10\", 3, 30.00),\n",
    "    (2, 104, \"2025-05-10\", 4, 45.00),\n",
    "    (1, 105, \"2025-05-10\", 2, 60.00),\n",
    "    (1, 105, \"2025-05-10\", 1, 15.00),\n",
    "    (1, 106, \"2025-05-10\", 2, 10.00),\n",
    "\n",
    "    # 2025-05-11\n",
    "    (1, 201, \"2025-05-11\", 1, 20.00),\n",
    "    (2, 201, \"2025-05-11\", 2, 40.00),\n",
    "    (2, 202, \"2025-05-11\", 2, 40.00),\n",
    "    (3, 203, \"2025-05-11\", 3, 35.00),\n",
    "    (1, 204, \"2025-05-11\", 1, 25.00),\n",
    "    (2, 205, \"2025-05-11\", 2, 30.00),\n",
    "    (1, 206, \"2025-05-11\", 4, 50.00),\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"sale_date\", \"quantity_sold\", \"total_sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "df.show()\n",
    "df.createOrReplaceTempView('sales')\n",
    "spark.sql(\"\"\"\n",
    "with cte1 as (\n",
    "select product_id,sale_date,sum(total_sales) as total_sales\n",
    "from sales\n",
    "group by product_id,sale_date),cte2 as (\n",
    "select product_id,sale_date,total_sales,row_number() over (order by total_sales desc) as rank from cte1)\n",
    "select product_id,sale_date,total_sales from cte2 where rank<6\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
